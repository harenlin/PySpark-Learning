{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MDD.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPEYYpM9gF1PR21qrxuTVeW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harenlin/PySpark-Learning/blob/main/MDD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "8J8Eor7mf_1-",
        "outputId": "f119b44a-ed1e-45e7-ac65-4b7267313128"
      },
      "source": [
        "!pip install pyspark\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('MDD').getOrCreate()\n",
        "cores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\n",
        "print(\"You are working with\", cores, \"core(s)\")\n",
        "spark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/db/e18cfd78e408de957821ec5ca56de1250645b05f8523d169803d8df35a64/pyspark-3.1.2.tar.gz (212.4MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4MB 74kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 16.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=8fa183357cbbe2f22e94118ce6a0c60ce3ddcf454afdd2384e9ff4bae744cfe5\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/1b/2c/30f43be2627857ab80062bef1527c0128f7b4070b6b2d02139\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n",
            "You are working with 1 core(s)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://2dc1805b6170:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>MDD</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f6fa336e4d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQIE_WLUogrU"
      },
      "source": [
        "# Spark's Immutability\n",
        "\n",
        "Before we get started, let's first take a moment to discuss the concept of Sparks Immutability. Spark DataFrames are immutable. What does that mean? Let's take a look at an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIFPWVk8og8a",
        "outputId": "5e61d242-5c68-4c22-87b9-25c0d088c000"
      },
      "source": [
        "names = spark.createDataFrame([('Haren', 'Lin'), ('Watson', 'Wang')], ['first_name', 'last_name'])\n",
        "print(names.show())\n",
        "print(names.rdd.id())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+---------+\n",
            "|first_name|last_name|\n",
            "+----------+---------+\n",
            "|     Haren|      Lin|\n",
            "|    Watson|     Wang|\n",
            "+----------+---------+\n",
            "\n",
            "None\n",
            "27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0k7FPkCohWo",
        "outputId": "f333d917-c9a1-4c3e-96ad-bcf9b1b2289d"
      },
      "source": [
        "# add a col\n",
        "from pyspark.sql.functions import *\n",
        "names = names.select(names.first_name, names.last_name, concat_ws(' ', names.first_name, names.last_name).alias('full_name'))\n",
        "print(names.show())\n",
        "print(names.rdd.id())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+---------+-----------+\n",
            "|first_name|last_name|  full_name|\n",
            "+----------+---------+-----------+\n",
            "|     Haren|      Lin|  Haren Lin|\n",
            "|    Watson|     Wang|Watson Wang|\n",
            "+----------+---------+-----------+\n",
            "\n",
            "None\n",
            "37\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1A4LdNVxp5hU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiXQtKCBpbkx",
        "outputId": "13c62e1b-4d61-4569-ed94-4f7fa9432d48"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = '/content/drive/My Drive/PySpark/Datasets/'\n",
        "videos = spark.read.csv(path + 'youtubevideos.csv', inferSchema=True, header=True)\n",
        "# data source: https://www.kaggle.com/datasnaek/youtube-new#USvideos.csv"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Bhuv9SIqCUL",
        "outputId": "cd5ef30b-8bb2-48bb-9c4d-7b9fceb9392e"
      },
      "source": [
        "print(videos.printSchema())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- video_id: string (nullable = true)\n",
            " |-- trending_date: string (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- channel_title: string (nullable = true)\n",
            " |-- category_id: string (nullable = true)\n",
            " |-- publish_time: string (nullable = true)\n",
            " |-- tags: string (nullable = true)\n",
            " |-- views: string (nullable = true)\n",
            " |-- likes: string (nullable = true)\n",
            " |-- dislikes: string (nullable = true)\n",
            " |-- comment_count: string (nullable = true)\n",
            " |-- thumbnail_link: string (nullable = true)\n",
            " |-- comments_disabled: string (nullable = true)\n",
            " |-- ratings_disabled: string (nullable = true)\n",
            " |-- video_error_or_removed: string (nullable = true)\n",
            " |-- description: string (nullable = true)\n",
            "\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raJQSoPSqUe1",
        "outputId": "9e6a8ea5-3a80-404f-ad77-f8bb0caece02"
      },
      "source": [
        "videos.show(5)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
            "|   video_id|trending_date|               title|       channel_title|category_id|        publish_time|                tags|  views| likes|dislikes|comment_count|      thumbnail_link|comments_disabled|ratings_disabled|video_error_or_removed|         description|\n",
            "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
            "|2kyS6SvSYSE|     17.14.11|WE WANT TO TALK A...|        CaseyNeistat|         22|2017-11-13T17:13:...|     SHANtell martin| 748374| 57527|    2966|        15954|https://i.ytimg.c...|            False|           False|                 False|SHANTELL'S CHANNE...|\n",
            "|1ZAPwfrtAFY|     17.14.11|The Trump Preside...|     LastWeekTonight|         24|2017-11-13T07:30:...|\"last week tonigh...|2418783| 97185|    6146|        12703|https://i.ytimg.c...|            False|           False|                 False|One year after th...|\n",
            "|5qpjK5DgCt4|     17.14.11|Racist Superman |...|        Rudy Mancuso|         23|2017-11-12T19:05:...|\"racist superman\"...|3191434|146033|    5339|         8181|https://i.ytimg.c...|            False|           False|                 False|WATCH MY PREVIOUS...|\n",
            "|puqaWrEC7tY|     17.14.11|Nickelback Lyrics...|Good Mythical Mor...|         24|2017-11-13T11:00:...|\"rhett and link\"|...| 343168| 10172|     666|         2146|https://i.ytimg.c...|            False|           False|                 False|Today we find out...|\n",
            "|d380meD0W0M|     17.14.11|I Dare You: GOING...|            nigahiga|         24|2017-11-12T18:01:...|\"ryan\"|\"higa\"|\"hi...|2095731|132235|    1989|        17518|https://i.ytimg.c...|            False|           False|                 False|I know it's been ...|\n",
            "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AiZs6v_ZTKs"
      },
      "source": [
        "# Manipulate Data Types\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wln5tn6RqWR6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 795
        },
        "outputId": "f8b68d73-dcc2-4ba5-e065-83f455577010"
      },
      "source": [
        "# Notice all vars are strings above....\n",
        "from pyspark.sql.functions import * \n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# type casting \n",
        "df = videos.withColumn(\"views\", videos[\"views\"].cast(IntegerType())) \\\n",
        "           .withColumn(\"likes\", videos[\"likes\"].cast(IntegerType())) \\\n",
        "           .withColumn(\"dislikes\", videos[\"dislikes\"].cast(IntegerType())) \\\n",
        "           .withColumn(\"trending_date\", to_date(videos.trending_date, 'dd.mm.yy')) \\\n",
        "#          .withColumn(\"publish_time\", to_timestamp(videos.publish_time, 'yyyy-MM-dd HH:mm:ss:ms'))\n",
        "print(df.printSchema())\n",
        "df.limit(4).toPandas()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- video_id: string (nullable = true)\n",
            " |-- trending_date: date (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- channel_title: string (nullable = true)\n",
            " |-- category_id: string (nullable = true)\n",
            " |-- publish_time: string (nullable = true)\n",
            " |-- tags: string (nullable = true)\n",
            " |-- views: integer (nullable = true)\n",
            " |-- likes: integer (nullable = true)\n",
            " |-- dislikes: integer (nullable = true)\n",
            " |-- comment_count: string (nullable = true)\n",
            " |-- thumbnail_link: string (nullable = true)\n",
            " |-- comments_disabled: string (nullable = true)\n",
            " |-- ratings_disabled: string (nullable = true)\n",
            " |-- video_error_or_removed: string (nullable = true)\n",
            " |-- description: string (nullable = true)\n",
            "\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_id</th>\n",
              "      <th>trending_date</th>\n",
              "      <th>title</th>\n",
              "      <th>channel_title</th>\n",
              "      <th>category_id</th>\n",
              "      <th>publish_time</th>\n",
              "      <th>tags</th>\n",
              "      <th>views</th>\n",
              "      <th>likes</th>\n",
              "      <th>dislikes</th>\n",
              "      <th>comment_count</th>\n",
              "      <th>thumbnail_link</th>\n",
              "      <th>comments_disabled</th>\n",
              "      <th>ratings_disabled</th>\n",
              "      <th>video_error_or_removed</th>\n",
              "      <th>description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2kyS6SvSYSE</td>\n",
              "      <td>2011-01-17</td>\n",
              "      <td>WE WANT TO TALK ABOUT OUR MARRIAGE</td>\n",
              "      <td>CaseyNeistat</td>\n",
              "      <td>22</td>\n",
              "      <td>2017-11-13T17:13:01.000Z</td>\n",
              "      <td>SHANtell martin</td>\n",
              "      <td>748374</td>\n",
              "      <td>57527</td>\n",
              "      <td>2966</td>\n",
              "      <td>15954</td>\n",
              "      <td>https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>SHANTELL'S CHANNEL - https://www.youtube.com/s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1ZAPwfrtAFY</td>\n",
              "      <td>2011-01-17</td>\n",
              "      <td>The Trump Presidency: Last Week Tonight with J...</td>\n",
              "      <td>LastWeekTonight</td>\n",
              "      <td>24</td>\n",
              "      <td>2017-11-13T07:30:00.000Z</td>\n",
              "      <td>\"last week tonight trump presidency\"|\"last wee...</td>\n",
              "      <td>2418783</td>\n",
              "      <td>97185</td>\n",
              "      <td>6146</td>\n",
              "      <td>12703</td>\n",
              "      <td>https://i.ytimg.com/vi/1ZAPwfrtAFY/default.jpg</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>One year after the presidential election, John...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5qpjK5DgCt4</td>\n",
              "      <td>2011-01-17</td>\n",
              "      <td>Racist Superman | Rudy Mancuso, King Bach &amp; Le...</td>\n",
              "      <td>Rudy Mancuso</td>\n",
              "      <td>23</td>\n",
              "      <td>2017-11-12T19:05:24.000Z</td>\n",
              "      <td>\"racist superman\"|\"rudy\"|\"mancuso\"|\"king\"|\"bac...</td>\n",
              "      <td>3191434</td>\n",
              "      <td>146033</td>\n",
              "      <td>5339</td>\n",
              "      <td>8181</td>\n",
              "      <td>https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► http...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>puqaWrEC7tY</td>\n",
              "      <td>2011-01-17</td>\n",
              "      <td>Nickelback Lyrics: Real or Fake?</td>\n",
              "      <td>Good Mythical Morning</td>\n",
              "      <td>24</td>\n",
              "      <td>2017-11-13T11:00:04.000Z</td>\n",
              "      <td>\"rhett and link\"|\"gmm\"|\"good mythical morning\"...</td>\n",
              "      <td>343168</td>\n",
              "      <td>10172</td>\n",
              "      <td>666</td>\n",
              "      <td>2146</td>\n",
              "      <td>https://i.ytimg.com/vi/puqaWrEC7tY/default.jpg</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>Today we find out if Link is a Nickelback amat...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      video_id  ...                                        description\n",
              "0  2kyS6SvSYSE  ...  SHANTELL'S CHANNEL - https://www.youtube.com/s...\n",
              "1  1ZAPwfrtAFY  ...  One year after the presidential election, John...\n",
              "2  5qpjK5DgCt4  ...  WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► http...\n",
              "3  puqaWrEC7tY  ...  Today we find out if Link is a Nickelback amat...\n",
              "\n",
              "[4 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPT_BHUWqWUa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edbd498e-f12c-41b2-c127-16de4380ff59"
      },
      "source": [
        "df.show(4)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
            "|   video_id|trending_date|               title|       channel_title|category_id|        publish_time|                tags|  views| likes|dislikes|comment_count|      thumbnail_link|comments_disabled|ratings_disabled|video_error_or_removed|         description|\n",
            "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
            "|2kyS6SvSYSE|   2011-01-17|WE WANT TO TALK A...|        CaseyNeistat|         22|2017-11-13T17:13:...|     SHANtell martin| 748374| 57527|    2966|        15954|https://i.ytimg.c...|            False|           False|                 False|SHANTELL'S CHANNE...|\n",
            "|1ZAPwfrtAFY|   2011-01-17|The Trump Preside...|     LastWeekTonight|         24|2017-11-13T07:30:...|\"last week tonigh...|2418783| 97185|    6146|        12703|https://i.ytimg.c...|            False|           False|                 False|One year after th...|\n",
            "|5qpjK5DgCt4|   2011-01-17|Racist Superman |...|        Rudy Mancuso|         23|2017-11-12T19:05:...|\"racist superman\"...|3191434|146033|    5339|         8181|https://i.ytimg.c...|            False|           False|                 False|WATCH MY PREVIOUS...|\n",
            "|puqaWrEC7tY|   2011-01-17|Nickelback Lyrics...|Good Mythical Mor...|         24|2017-11-13T11:00:...|\"rhett and link\"|...| 343168| 10172|     666|         2146|https://i.ytimg.c...|            False|           False|                 False|Today we find out...|\n",
            "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
            "only showing top 4 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU3CBuE6qWcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7417e0dc-55a9-437f-c46a-cef50785f04e"
      },
      "source": [
        "# Simple Rename\n",
        "renamed = df.withColumnRenamed('channel_title', 'channel_title_new')\n",
        "renamed.show(5)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
            "|   video_id|trending_date|               title|   channel_title_new|category_id|        publish_time|                tags|  views| likes|dislikes|comment_count|      thumbnail_link|comments_disabled|ratings_disabled|video_error_or_removed|         description|\n",
            "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
            "|2kyS6SvSYSE|   2011-01-17|WE WANT TO TALK A...|        CaseyNeistat|         22|2017-11-13T17:13:...|     SHANtell martin| 748374| 57527|    2966|        15954|https://i.ytimg.c...|            False|           False|                 False|SHANTELL'S CHANNE...|\n",
            "|1ZAPwfrtAFY|   2011-01-17|The Trump Preside...|     LastWeekTonight|         24|2017-11-13T07:30:...|\"last week tonigh...|2418783| 97185|    6146|        12703|https://i.ytimg.c...|            False|           False|                 False|One year after th...|\n",
            "|5qpjK5DgCt4|   2011-01-17|Racist Superman |...|        Rudy Mancuso|         23|2017-11-12T19:05:...|\"racist superman\"...|3191434|146033|    5339|         8181|https://i.ytimg.c...|            False|           False|                 False|WATCH MY PREVIOUS...|\n",
            "|puqaWrEC7tY|   2011-01-17|Nickelback Lyrics...|Good Mythical Mor...|         24|2017-11-13T11:00:...|\"rhett and link\"|...| 343168| 10172|     666|         2146|https://i.ytimg.c...|            False|           False|                 False|Today we find out...|\n",
            "|d380meD0W0M|   2011-01-17|I Dare You: GOING...|            nigahiga|         24|2017-11-12T18:01:...|\"ryan\"|\"higa\"|\"hi...|2095731|132235|    1989|        17518|https://i.ytimg.c...|            False|           False|                 False|I know it's been ...|\n",
            "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2AU7tr7Z-D6"
      },
      "source": [
        "**Clean Data**\n",
        "\n",
        "Alright so we see that the publish_time variable could not be converted to a timestamp becuase it has those strange \"T\" and \"Z\" values between the date and the time. We essentially need to replace the \"T\" value with a space, and the Z value with nothing. There are a couple of ways we can do this, the first is regex which is short for regular expressions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaYGGHZSaDup"
      },
      "source": [
        "**Regex**\n",
        "\n",
        "Regex is used to replace or extract all substrings of the specified string value that match regexp with repetition.\n",
        "\n",
        "The syntax here is: regexp_replace(*str, pattern, replacement*)\n",
        "\n",
        "Regex is NOT super intuitive, so if you need a refresher on regex calls visit: \n",
        " - https://www.whoishostingthis.com/resources/regex/\n",
        " - https://docs.oracle.com/cd/B19306_01/server.102/b14200/ap_posix001.htm#BABJDBHB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Kz1ZhBvZ3nC",
        "outputId": "71150006-8e3f-4d52-fb6b-c93599aa488f"
      },
      "source": [
        "from pyspark.sql.functions import regexp_replace#, regexp_extract\n",
        "\n",
        "df = df.withColumn('publish_time_2', regexp_replace(df.publish_time, 'T', ' '))\n",
        "df = df.withColumn('publish_time_2', regexp_replace(df.publish_time_2, 'Z', ''))\n",
        "df = df.withColumn(\"publish_time_3\", to_timestamp(df.publish_time_2, 'yyyy-MM-dd HH:mm:ss.SSS'))\n",
        "# print(df.printSchema())\n",
        "df.select(\"publish_time\", \"publish_time_2\", \"publish_time_3\").show(5,False)\n",
        "# Notice the .000 on the end of publish_time_new as opposed to publish_time_new_t"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------+-----------------------+-------------------+\n",
            "|publish_time            |publish_time_2         |publish_time_3     |\n",
            "+------------------------+-----------------------+-------------------+\n",
            "|2017-11-13T17:13:01.000Z|2017-11-13 17:13:01.000|2017-11-13 17:13:01|\n",
            "|2017-11-13T07:30:00.000Z|2017-11-13 07:30:00.000|2017-11-13 07:30:00|\n",
            "|2017-11-12T19:05:24.000Z|2017-11-12 19:05:24.000|2017-11-12 19:05:24|\n",
            "|2017-11-13T11:00:04.000Z|2017-11-13 11:00:04.000|2017-11-13 11:00:04|\n",
            "|2017-11-12T18:01:41.000Z|2017-11-12 18:01:41.000|2017-11-12 18:01:41|\n",
            "+------------------------+-----------------------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvzNBbrWaWCO"
      },
      "source": [
        "**Translate Function**\n",
        "\n",
        "You could also use the Translate function here to do this, where the first set of values is what you are looking for and the second set is what you want to replace those values with respectively. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUVe-pTAaKop",
        "outputId": "640bbac6-a011-45b0-aa98-dbcfc6aaa3c7"
      },
      "source": [
        "import pyspark.sql.functions as F\n",
        "df.select(\"publish_time\", F.translate(F.col(\"publish_time\"), \"TZ\", \" \").alias(\"translate_func_time\")).show(5,False)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------+-----------------------+\n",
            "|publish_time            |translate_func_time    |\n",
            "+------------------------+-----------------------+\n",
            "|2017-11-13T17:13:01.000Z|2017-11-13 17:13:01.000|\n",
            "|2017-11-13T07:30:00.000Z|2017-11-13 07:30:00.000|\n",
            "|2017-11-12T19:05:24.000Z|2017-11-12 19:05:24.000|\n",
            "|2017-11-13T11:00:04.000Z|2017-11-13 11:00:04.000|\n",
            "|2017-11-12T18:01:41.000Z|2017-11-12 18:01:41.000|\n",
            "+------------------------+-----------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylEv1j3EagTR"
      },
      "source": [
        "**Trim**\n",
        "\n",
        "One common function you've probably seen in almost any data processing tool including excel is the \"trim\" function which removes leading and trailing white space from a cell in various ways. Let's go ahead and do that with the title field."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTfZcmZWabXq",
        "outputId": "32733917-1195-4792-e026-6c51a4c55e57"
      },
      "source": [
        "# Trim\n",
        "# pyspark.sql.functions.trim(col) - Trim the spaces from both ends for the specified string column.\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "df = df.withColumn('title', trim(df.title)) # or rtrim/ltrim\n",
        "df.select(\"title\").show(5,False)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------------------+\n",
            "|title                                                         |\n",
            "+--------------------------------------------------------------+\n",
            "|WE WANT TO TALK ABOUT OUR MARRIAGE                            |\n",
            "|The Trump Presidency: Last Week Tonight with John Oliver (HBO)|\n",
            "|Racist Superman | Rudy Mancuso, King Bach & Lele Pons         |\n",
            "|Nickelback Lyrics: Real or Fake?                              |\n",
            "|I Dare You: GOING BALD!?                                      |\n",
            "+--------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GEe0CkbahKE",
        "outputId": "6ee822b0-5bbc-4310-b036-90dca9b3cd29"
      },
      "source": [
        "trim_ex = spark.createDataFrame([(' 2015-04-08 ',' 2015-05-10 ')], ['d1', 'd2']) # create a dataframe - notice the extra whitespaces in the date strings\n",
        "trim_ex.show()\n",
        "print(\"left trim\")\n",
        "trim_ex.select('d1', ltrim(trim_ex.d1)).show()\n",
        "print(\"right trim\")\n",
        "trim_ex.select('d1', rtrim(trim_ex.d1)).show()\n",
        "print(\"trim\")\n",
        "trim_ex.select('d1', trim(trim_ex.d1)).show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+------------+\n",
            "|          d1|          d2|\n",
            "+------------+------------+\n",
            "| 2015-04-08 | 2015-05-10 |\n",
            "+------------+------------+\n",
            "\n",
            "left trim\n",
            "+------------+-----------+\n",
            "|          d1|  ltrim(d1)|\n",
            "+------------+-----------+\n",
            "| 2015-04-08 |2015-04-08 |\n",
            "+------------+-----------+\n",
            "\n",
            "right trim\n",
            "+------------+-----------+\n",
            "|          d1|  rtrim(d1)|\n",
            "+------------+-----------+\n",
            "| 2015-04-08 | 2015-04-08|\n",
            "+------------+-----------+\n",
            "\n",
            "trim\n",
            "+------------+----------+\n",
            "|          d1|  trim(d1)|\n",
            "+------------+----------+\n",
            "| 2015-04-08 |2015-04-08|\n",
            "+------------+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvKbd-ysahMP",
        "outputId": "5d4d8438-ab6a-4e37-d65c-f1871bd2bd6b"
      },
      "source": [
        "# lower\n",
        "df = df.withColumn('title', lower(df.title))\n",
        "df.select('title').show(5,False)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------------------+\n",
            "|title                                                         |\n",
            "+--------------------------------------------------------------+\n",
            "|we want to talk about our marriage                            |\n",
            "|the trump presidency: last week tonight with john oliver (hbo)|\n",
            "|racist superman | rudy mancuso, king bach & lele pons         |\n",
            "|nickelback lyrics: real or fake?                              |\n",
            "|i dare you: going bald!?                                      |\n",
            "+--------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7KrOjJja9f7"
      },
      "source": [
        "# Case When\n",
        "\n",
        "We can also use the classic sql \"case when\" clause to recode values. Let's say we wanted to create a categorical variable that told if the video had more likes than dislikes and visa versa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGiVS9NNahOq",
        "outputId": "8057bb96-661a-4e1e-d194-2229d59392b9"
      },
      "source": [
        "print(\"Option#1: select or withColumn() using when-otherwise\")\n",
        "from pyspark.sql.functions import when\n",
        "df.select(\"likes\", \"dislikes\", ( when(df.likes > df.dislikes, 'Good').when(df.likes < df.dislikes, 'Bad').otherwise('Undetermined') ).alias(\"Favorability\")).show(3)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Option#1: select or withColumn() using when-otherwise\n",
            "+------+--------+------------+\n",
            "| likes|dislikes|Favorability|\n",
            "+------+--------+------------+\n",
            "| 57527|    2966|        Good|\n",
            "| 97185|    6146|        Good|\n",
            "|146033|    5339|        Good|\n",
            "+------+--------+------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxyZ68DvbEUM",
        "outputId": "266fdb68-96b3-4675-ee44-3dbef562d8da"
      },
      "source": [
        "print(\"Option#2: select or withColumn() using expr function\")\n",
        "from pyspark.sql.functions import expr \n",
        "df.select(\"likes\", \"dislikes\",\n",
        "          expr(\"CASE WHEN likes > dislikes THEN 'Good' WHEN likes < dislikes THEN 'Bad' ELSE 'Undetermined' END AS Favorability\")).show(3)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Option#2: select or withColumn() using expr function\n",
            "+------+--------+------------+\n",
            "| likes|dislikes|Favorability|\n",
            "+------+--------+------------+\n",
            "| 57527|    2966|        Good|\n",
            "| 97185|    6146|        Good|\n",
            "|146033|    5339|        Good|\n",
            "+------+--------+------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0JgtbQAbFjh",
        "outputId": "6fd2a771-23dd-4296-e486-9351d6a4b80f"
      },
      "source": [
        "print(\"Option#3: selectExpr() using SQL equivalent CASE expression\")\n",
        "df.selectExpr(\"likes\", \"dislikes\", \"title\", \"CASE WHEN likes > dislikes THEN  'Good' WHEN likes < dislikes THEN 'Bad' ELSE 'Undetermined' END AS Favorability\").show(3,False)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Option#3: selectExpr() using SQL equivalent CASE expression\n",
            "+------+--------+--------------------------------------------------------------+------------+\n",
            "|likes |dislikes|title                                                         |Favorability|\n",
            "+------+--------+--------------------------------------------------------------+------------+\n",
            "|57527 |2966    |we want to talk about our marriage                            |Good        |\n",
            "|97185 |6146    |the trump presidency: last week tonight with john oliver (hbo)|Good        |\n",
            "|146033|5339    |racist superman | rudy mancuso, king bach & lele pons         |Good        |\n",
            "+------+--------+--------------------------------------------------------------+------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQZvWu09CSg_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86fd7ebe-b6ef-4eed-cc91-3ca120bb42f0"
      },
      "source": [
        "print(\"Option#1: select or withColumn() using when-otherwise\")\n",
        "from pyspark.sql.functions import when\n",
        "df.select(\"likes\", \"dislikes\", (when(df.likes > df.dislikes, 'Good').when(df.likes < df.dislikes, 'Bad').otherwise('Undetermined')).alias(\"Favorability\")).show(5)\n",
        "\n",
        "print(\"Option#2: select or withColumn() using expr function\")\n",
        "from pyspark.sql.functions import expr \n",
        "df.select(\"likes\", \"dislikes\", expr(\"CASE WHEN likes > dislikes THEN 'Good' WHEN likes < dislikes THEN 'Bad' ELSE 'Undetermined' END AS Favorability\")).show(5)\n",
        "\n",
        "print(\"Option#3: selectExpr() using SQL equivalent CASE expression\")\n",
        "df.selectExpr(\"likes\", \"dislikes\", \"CASE WHEN likes > dislikes THEN  'Good' WHEN likes < dislikes THEN 'Bad' ELSE 'Undetermined' END AS Favorability\").show(5)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Option#1: select or withColumn() using when-otherwise\n",
            "+------+--------+------------+\n",
            "| likes|dislikes|Favorability|\n",
            "+------+--------+------------+\n",
            "| 57527|    2966|        Good|\n",
            "| 97185|    6146|        Good|\n",
            "|146033|    5339|        Good|\n",
            "| 10172|     666|        Good|\n",
            "|132235|    1989|        Good|\n",
            "+------+--------+------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Option#2: select or withColumn() using expr function\n",
            "+------+--------+------------+\n",
            "| likes|dislikes|Favorability|\n",
            "+------+--------+------------+\n",
            "| 57527|    2966|        Good|\n",
            "| 97185|    6146|        Good|\n",
            "|146033|    5339|        Good|\n",
            "| 10172|     666|        Good|\n",
            "|132235|    1989|        Good|\n",
            "+------+--------+------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Option#3: selectExpr() using SQL equivalent CASE expression\n",
            "+------+--------+------------+\n",
            "| likes|dislikes|Favorability|\n",
            "+------+--------+------------+\n",
            "| 57527|    2966|        Good|\n",
            "| 97185|    6146|        Good|\n",
            "|146033|    5339|        Good|\n",
            "| 10172|     666|        Good|\n",
            "|132235|    1989|        Good|\n",
            "+------+--------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSfIy7BrgDVe"
      },
      "source": [
        "**Concatenate**\n",
        "\n",
        "If you want to combine two variables together (given a separator) you can use the concatenate method. Let's say we wanted to combined all the text description variables of the videos here for a robust NLP exercise of some sort and we needed to have all the text in one colum to do that like this.\n",
        "\n",
        "    concat_ws(sep, *cols)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fedsnfxFgTvx",
        "outputId": "c079b989-60e8-4f6a-a42a-603b11f5d701"
      },
      "source": [
        "df.select(df.title, df.channel_title).show(5)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+\n",
            "|               title|       channel_title|\n",
            "+--------------------+--------------------+\n",
            "|we want to talk a...|        CaseyNeistat|\n",
            "|the trump preside...|     LastWeekTonight|\n",
            "|racist superman |...|        Rudy Mancuso|\n",
            "|nickelback lyrics...|Good Mythical Mor...|\n",
            "|i dare you: going...|            nigahiga|\n",
            "+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAoWsVaIbGrQ",
        "outputId": "f9dc02f3-4337-465c-9df4-6c42c9662d18"
      },
      "source": [
        "from pyspark.sql.functions import concat_ws # concat with separator\n",
        "df.select( df.title, df.channel_title, concat_ws(' || ', df.title, df.channel_title).alias('text') ).show(5,False)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------------------+---------------------+---------------------------------------------------------------------------------+\n",
            "|title                                                         |channel_title        |text                                                                             |\n",
            "+--------------------------------------------------------------+---------------------+---------------------------------------------------------------------------------+\n",
            "|we want to talk about our marriage                            |CaseyNeistat         |we want to talk about our marriage || CaseyNeistat                               |\n",
            "|the trump presidency: last week tonight with john oliver (hbo)|LastWeekTonight      |the trump presidency: last week tonight with john oliver (hbo) || LastWeekTonight|\n",
            "|racist superman | rudy mancuso, king bach & lele pons         |Rudy Mancuso         |racist superman | rudy mancuso, king bach & lele pons || Rudy Mancuso            |\n",
            "|nickelback lyrics: real or fake?                              |Good Mythical Morning|nickelback lyrics: real or fake? || Good Mythical Morning                        |\n",
            "|i dare you: going bald!?                                      |nigahiga             |i dare you: going bald!? || nigahiga                                             |\n",
            "+--------------------------------------------------------------+---------------------+---------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZcAVbp4gjW-"
      },
      "source": [
        "**Extracting data from Date and Timestamp variables**\n",
        "\n",
        "If you have the need to extract say the year or month from a date field, you can use PySpark's SQL function library like this. \n",
        "\n",
        "Note with this analysis we stumbled apon a date conversion descrepancy here. I'll leave fixing that for a hw problem!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faTv-T0bgaru",
        "outputId": "3dc75e62-a1d9-42d7-b4f8-f4e8d07dc894"
      },
      "source": [
        "from pyspark.sql.functions import year, month\n",
        "# Other options: dayofmonth, dayofweek, dayofyear, weekofyear\n",
        "df.select(\"trending_date\", year(\"trending_date\"), month(\"trending_date\")).show(5)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+-------------------+--------------------+\n",
            "|trending_date|year(trending_date)|month(trending_date)|\n",
            "+-------------+-------------------+--------------------+\n",
            "|   2011-01-17|               2011|                   1|\n",
            "|   2011-01-17|               2011|                   1|\n",
            "|   2011-01-17|               2011|                   1|\n",
            "|   2011-01-17|               2011|                   1|\n",
            "|   2011-01-17|               2011|                   1|\n",
            "+-------------+-------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4B5kbFkMgqGD"
      },
      "source": [
        "**Calculate the Difference between two dates**\n",
        "\n",
        "If you want to calculate the time difference between two dates, you could use PySparks datediff function which returns the number of days from start to end.\n",
        "\n",
        "    datediff(end, start)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihO9sGbGgRkF",
        "outputId": "f076985f-aaf6-4fe5-85df-0943557325a4"
      },
      "source": [
        "from pyspark.sql.functions import datediff\n",
        "df.select(\"trending_date\", \"publish_time_3\", (datediff(df.trending_date,df.publish_time_3)/365).alias('diff')).show(5)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+-------------------+-------------------+\n",
            "|trending_date|     publish_time_3|               diff|\n",
            "+-------------+-------------------+-------------------+\n",
            "|   2011-01-17|2017-11-13 17:13:01|-6.8273972602739725|\n",
            "|   2011-01-17|2017-11-13 07:30:00|-6.8273972602739725|\n",
            "|   2011-01-17|2017-11-12 19:05:24| -6.824657534246575|\n",
            "|   2011-01-17|2017-11-13 11:00:04|-6.8273972602739725|\n",
            "|   2011-01-17|2017-11-12 18:01:41| -6.824657534246575|\n",
            "+-------------+-------------------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox9_KGfWgat_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0xqMqcqgawF",
        "outputId": "0a947559-414e-485c-b11c-3640f5c5e6af"
      },
      "source": [
        "# Split a string around pattern (pattern is a regular expression).\n",
        "from pyspark.sql.functions import split\n",
        "\n",
        "df.select(\"title\").show(5,False)\n",
        "df.select(df.title, split(df.title, ' ').alias('title_split')).show(5,False)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------------------+\n",
            "|title                                                         |\n",
            "+--------------------------------------------------------------+\n",
            "|we want to talk about our marriage                            |\n",
            "|the trump presidency: last week tonight with john oliver (hbo)|\n",
            "|racist superman | rudy mancuso, king bach & lele pons         |\n",
            "|nickelback lyrics: real or fake?                              |\n",
            "|i dare you: going bald!?                                      |\n",
            "+--------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+--------------------------------------------------------------+-------------------------------------------------------------------------+\n",
            "|title                                                         |title_split                                                              |\n",
            "+--------------------------------------------------------------+-------------------------------------------------------------------------+\n",
            "|we want to talk about our marriage                            |[we, want, to, talk, about, our, marriage]                               |\n",
            "|the trump presidency: last week tonight with john oliver (hbo)|[the, trump, presidency:, last, week, tonight, with, john, oliver, (hbo)]|\n",
            "|racist superman | rudy mancuso, king bach & lele pons         |[racist, superman, |, rudy, mancuso,, king, bach, &, lele, pons]         |\n",
            "|nickelback lyrics: real or fake?                              |[nickelback, lyrics:, real, or, fake?]                                   |\n",
            "|i dare you: going bald!?                                      |[i, dare, you:, going, bald!?]                                           |\n",
            "+--------------------------------------------------------------+-------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlwpBhi4gayP",
        "outputId": "053a6554-4f6b-4f01-e98b-a23b4aa5d5c8"
      },
      "source": [
        "from pyspark.sql.functions import *\n",
        "array_df = df.select(\"title\", split(df.title, ' ').alias('title_array'))\n",
        "array_df.select(\"title\", array_contains(array_df.title_array, \"marriage\")).show(5,False)\n",
        "\n",
        "# get rid of repeat values\n",
        "array_df.select(array_distinct(array_df.title_array)).show(5,False)\n",
        "\n",
        "# remove certian values\n",
        "array_df.select(array_remove(array_df.title_array, \"we\")).show(5,False)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------------------+-------------------------------------+\n",
            "|title                                                         |array_contains(title_array, marriage)|\n",
            "+--------------------------------------------------------------+-------------------------------------+\n",
            "|we want to talk about our marriage                            |true                                 |\n",
            "|the trump presidency: last week tonight with john oliver (hbo)|false                                |\n",
            "|racist superman | rudy mancuso, king bach & lele pons         |false                                |\n",
            "|nickelback lyrics: real or fake?                              |false                                |\n",
            "|i dare you: going bald!?                                      |false                                |\n",
            "+--------------------------------------------------------------+-------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-------------------------------------------------------------------------+\n",
            "|array_distinct(title_array)                                              |\n",
            "+-------------------------------------------------------------------------+\n",
            "|[we, want, to, talk, about, our, marriage]                               |\n",
            "|[the, trump, presidency:, last, week, tonight, with, john, oliver, (hbo)]|\n",
            "|[racist, superman, |, rudy, mancuso,, king, bach, &, lele, pons]         |\n",
            "|[nickelback, lyrics:, real, or, fake?]                                   |\n",
            "|[i, dare, you:, going, bald!?]                                           |\n",
            "+-------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-------------------------------------------------------------------------+\n",
            "|array_remove(title_array, we)                                            |\n",
            "+-------------------------------------------------------------------------+\n",
            "|[want, to, talk, about, our, marriage]                                   |\n",
            "|[the, trump, presidency:, last, week, tonight, with, john, oliver, (hbo)]|\n",
            "|[racist, superman, |, rudy, mancuso,, king, bach, &, lele, pons]         |\n",
            "|[nickelback, lyrics:, real, or, fake?]                                   |\n",
            "|[i, dare, you:, going, bald!?]                                           |\n",
            "+-------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj4uAbEaiNkz"
      },
      "source": [
        "## Creating Functions\n",
        "\n",
        "Functions as you know them in Python work a bit differently in Pyspark because it operates on a cluster. If you define a function the traditional Python way in PySpark, you will not recieve an error message but the call will not distribute on all nodes. So it will run slower. \n",
        "\n",
        "So to convert a Python function to what's called a user defined function (UDF) in PySpark. This is what you do.\n",
        "\n",
        "*Note: keep in mind that a function will not work on a column with null values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKxKc3DHhJpR",
        "outputId": "be752aab-6843-4d06-8573-80bb786d6cd0"
      },
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "# step1: define function\n",
        "def square(x):\n",
        "    return int(x**2)\n",
        "# step2: convert function to udf and define a new name\n",
        "square_udf = udf(lambda x: square(x), IntegerType())\n",
        "\n",
        "df.select('dislikes', square_udf('dislikes').alias('dislikes_sq')).where(col('dislikes').isNotNull()).show(5)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-----------+\n",
            "|dislikes|dislikes_sq|\n",
            "+--------+-----------+\n",
            "|    2966|    8797156|\n",
            "|    6146|   37773316|\n",
            "|    5339|   28504921|\n",
            "|     666|     443556|\n",
            "|    1989|    3956121|\n",
            "+--------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aggnke3su4Nd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dd615b9-c5fc-4fb8-e845-d316b925f203"
      },
      "source": [
        "@udf(returnType=IntegerType())\n",
        "def udfsquare(x):\n",
        "    return int(x**2)\n",
        "\n",
        "df.select('dislikes', udfsquare('dislikes').alias('dislikes_sq')).where(col('dislikes').isNotNull()).show(5)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-----------+\n",
            "|dislikes|dislikes_sq|\n",
            "+--------+-----------+\n",
            "|    2966|    8797156|\n",
            "|    6146|   37773316|\n",
            "|    5339|   28504921|\n",
            "|     666|     443556|\n",
            "|    1989|    3956121|\n",
            "+--------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}